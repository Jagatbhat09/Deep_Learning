{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro DL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParameters:\n",
    "    def __init__(self, learning_rate=0.01, epochs=10, mini_batch_size=None, beta=.9, layers=None, beta1=.9, beta2=.998):\n",
    "        if layers is None:\n",
    "            layers = [10, 20, 10]\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = mini_batch_size\n",
    "        self.beta = beta\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "\n",
    "\n",
    "class WeightAndBias: \n",
    "    def __init__(self,number_features, layers, initialisation_type=\"random\"):\n",
    "        self.initialisation_type = initialisation_type\n",
    "        self.layers = [number_features]+ layers\n",
    "        \n",
    "        self.weights = [pd.DataFrame()] + [np.random.randn(self.layers[i+1], self.layers[i]) * 0.01 \n",
    "                                           for i in range(len(self.layers)-1)]\n",
    "        \n",
    "        self.biases = [pd.DataFrame()] + [np.zeros([self.layers[i+1], 1]) for i in range(len(self.layers)-1)]\n",
    "        \n",
    "        print([(self.layers[i+1], self.layers[i]) for i in range(len(self.layers)-1)])\n",
    "        \n",
    "    def update_learning_parameters(self):\n",
    "        pass\n",
    "    \n",
    "\n",
    "class ActivationFunctions:\n",
    "    def __init__(self, layers, activation_functions=None) :\n",
    "        if activation_functions is None: \n",
    "            activation_functions= ['tanh'] * (len(layers) - 1) + ['softmax']\n",
    "            \n",
    "        self.activation_functions = [None] + [eval(f'ActivationFunctions.{activation_function}') \n",
    "                                     for activation_function in activation_functions]\n",
    "        \n",
    "        self.derivative_functions = [None] + [eval(f'ActivationFunctions.{activation_function}_derivative') \n",
    "                                     for activation_function in activation_functions]\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(z) :\n",
    "        return 1 / (1 + np.exp( -z ))\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(z) : \n",
    "        return np.where(z>0, z, 0.0001 * z )\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(z) :\n",
    "        # return np.tanh(z\n",
    "        z = np.clip(z, -20, 20)\n",
    "        return (np.exp(z) - np.exp(-z))/ (np.exp(z) + np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(z):\n",
    "        z = np.clip(z, -20, 20)\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis=0) \n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax_derivative(y, a) :\n",
    "        return a - y\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(y, a) :\n",
    "        return a - y\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh_derivative(z) :\n",
    "        return (1 - np.tanh(z) ** 2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(z) :\n",
    "        return (z > 0) * 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_loss(a, y, m) :\n",
    "        return -1/m * np.sum(np.multiply(y, np.log(a)))\n",
    "    \n",
    "\n",
    "class NeuralNetwork: \n",
    "    def __init__(self, X_train, y_train, HyperParameters, activation_functions=None) :\n",
    "        \n",
    "        self.X_train, self.y_train = X_train, y_train\n",
    "        self.n, self.m = X_train.shape #number of training examples (m) , number of features (n)\n",
    "        \n",
    "        print(f\"number of training examples: {self.m}\\nnumber of features: {self.n}\"\n",
    "              f\"shape of y_train {self.y_train.shape}\")\n",
    "        # self.input_data = InputData(X, y)\n",
    "        self.hp = HyperParameters\n",
    "        self.layers = self.hp.layers\n",
    "        self.no_l = len(self.layers)\n",
    "        \n",
    "        self.act_function_obj = ActivationFunctions(self.layers, activation_functions=activation_functions)\n",
    " \n",
    "        self.lp = WeightAndBias(self.n, self.layers) #lp --> learning parameters\n",
    "            \n",
    "    # this will be put under weights class later\n",
    "    def update_learning_parameters(self) :\n",
    "        for l in range(1, self.no_l+1):\n",
    "            self.lp.biases[l] = self.lp.biases[l] - self.hp.learning_rate * self.db[l]\n",
    "            self.lp.weights[l] = self.lp.weights[l] - self.hp.learning_rate * self.dW[l]\n",
    "\n",
    "    def forward_propagation(self, X_test=None) :\n",
    "        ## the way the forward prop is called needs to be changed\n",
    "        if X_test is not None :\n",
    "            X = X_test\n",
    "        else: \n",
    "            X = self.X_train\n",
    "        self.Z, self.A = [0] + [None] * self.no_l, [X] + [None] * self.no_l\n",
    "        activation_functions = self.act_function_obj.activation_functions\n",
    "        \n",
    "        for l in range(1, self.no_l + 1):\n",
    "            self.Z[l] = np.dot(self.lp.weights[l], self.A[l-1]) + self.lp.biases[l]\n",
    "            self.A[l] = activation_functions[l](self.Z[l])      \n",
    "\n",
    "            \n",
    "    def back_propagation(self) :\n",
    "\n",
    "        derivative_functions = self.act_function_obj.derivative_functions\n",
    "        self.dZ =[None] +  [None] * self.no_l\n",
    "        self.dW =[None] +  [None] * self.no_l\n",
    "        self.db =[None] +  [None] * self.no_l\n",
    "        \n",
    "        self.dZ[self.no_l] = derivative_functions[self.no_l](self.y_train, self.A[self.no_l])\n",
    "        self.dW[self.no_l] = 1/self.m * np.dot(self.dZ[self.no_l] , self.A[self.no_l - 1].T)\n",
    "        self.db[self.no_l] = 1/self.m * np.sum(self.dZ[self.no_l], axis=1, keepdims=True)\n",
    "        \n",
    "\n",
    "        assert self.dZ[self.no_l].shape == self.Z[self.no_l].shape\n",
    "        assert self.db[self.no_l].shape == self.lp.biases[self.no_l].shape        \n",
    "        assert self.dW[self.no_l].shape == self.lp.weights[self.no_l].shape\n",
    "        \n",
    "        for l in range(self.no_l - 1, 0, -1) : \n",
    "        \n",
    "            self.dZ[l] = np.dot(self.lp.weights[l+1].T, self.dZ[l+1] )* derivative_functions[l](self.Z[l])\n",
    "            self.dW[l] = 1/self.m * np.dot(self.dZ[l], self.A[l-1].T)\n",
    "            self.db[l] = 1/self.m * np.sum(self.dZ[l], axis=1, keepdims=True)\n",
    "            \n",
    "            assert self.dZ[l].shape == self.Z[l].shape\n",
    "            assert self.dW[l].shape == self.lp.weights[l].shape\n",
    "            assert self.db[l].shape == self.lp.biases[l].shape       \n",
    "        \n",
    "        \n",
    "    def train_nn(self, verbose=False, per_epoch_log=100) :\n",
    "        for epoch in range(self.hp.epochs): \n",
    "                self.forward_propagation()\n",
    "\n",
    "                if verbose and epoch % per_epoch_log == 0: \n",
    "                    print(f\"epochs {epoch} loss: \",ActivationFunctions.calculate_loss(self.A[self.no_l], self.y_train, self.m))\n",
    "\n",
    "                self.back_propagation()\n",
    "\n",
    "                self.update_learning_parameters()\n",
    "\n",
    "        print(f\"epochs {epoch} loss: \", ActivationFunctions.calculate_loss(self.A[self.no_l], self.y_train, self.m))\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        self.forward_propagation(X_test=X_test)\n",
    "        preds=  self.A[self.no_l].T\n",
    "        return (preds == preds.max(axis=1)[:,None]).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples: 4096\n",
      "number of features: 784shape of y_train (10, 4096)\n",
      "[(256, 784), (10, 256)]\n"
     ]
    }
   ],
   "source": [
    "m = 4096 # train_data.shape[0]\n",
    "test_samples = 10\n",
    "X = train_data.drop('label', axis=1).iloc[0:m] / 255\n",
    "X_test = train_data.drop('label', axis=1).iloc[m:m + test_samples] / 255\n",
    "y_test = train_data.label.iloc[m:m + test_samples]\n",
    "\n",
    "\n",
    "def one_hot_encoding_y(train_data) :\n",
    "    \n",
    "    a = train_data.label\n",
    "    b = np.zeros((a.size, 10))\n",
    "    b[np.arange(a.size),a] = 1\n",
    "    return b\n",
    "y  = one_hot_encoding_y(train_data)[:m]\n",
    "y = np.reshape(y, (m, 10))\n",
    "y_test = one_hot_encoding_y(train_data)[m:m + test_samples]\n",
    "y_test = np.reshape(y_test, (test_samples, 10))\n",
    "\n",
    "layers=[100, 15, 10]\n",
    "layers= [256, 10]\n",
    "activation_functions = ['tanh'] * (len(layers) - 1) + ['softmax']\n",
    "\n",
    "nn = NeuralNetwork(X.T, y.T, HyperParameters(layers=layers, learning_rate=0.5, epochs=100),\n",
    "                   activation_functions=activation_functions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 0 loss:  0.28413482305468574\n",
      "epochs 50 loss:  0.2348511969028756\n",
      "epochs 99 loss:  0.20196587701948127\n"
     ]
    }
   ],
   "source": [
    "nn.train_nn(verbose=True, per_epoch_log=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of examples: 4096\n",
      "number of right predictions: 3880\n",
      "number of wrong predictions: 216\n",
      "accuracy on train: 94.7265625\n"
     ]
    }
   ],
   "source": [
    "prob_preds = lambda preds: (preds == preds.max(axis=1)[:,None]).astype(int)\n",
    "r = np.sum(np.argmax(y, axis=1) == np.argmax(prob_preds(nn.A[2].T), axis=1))\n",
    "w = np.sum(np.argmax(y, axis=1) != np.argmax(prob_preds(nn.A[2].T), axis=1))\n",
    "print(f\"total number of examples: {m}\\nnumber of right predictions: {r}\\nnumber of wrong predictions: {w}\\n\"\n",
    "     f\"accuracy on train: {r/m * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 1)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = nn.predict(X_test.T)\n",
    "r = np.sum(np.argmax(y_test, axis=1) == np.argmax(prob_preds(nn.A[2].T), axis=1))\n",
    "w = np.sum(np.argmax(y_test, axis=1) != np.argmax(prob_preds(nn.A[2].T), axis=1))\n",
    "r, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
